{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MissOh Datasets\n",
    "### set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "from torchvision.transforms import Compose, Resize\n",
    "from PIL import Image\n",
    "import json\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Yolo_v2_pytorch.src.utils import *\n",
    "\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "(39, 129, 113)\n"
     ]
    }
   ],
   "source": [
    "MissOh_CLASSES = ['person']\n",
    "print(MissOh_CLASSES[0])\n",
    "global colors\n",
    "colors = pickle.load(open(\"../Yolo_v2_pytorch/src/pallete\", \"rb\"))\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        \"You Only Look Once:Unified, Real-Time Object Detection\")\n",
    "    parser.add_argument(\"--image_size\", type=int,\n",
    "                        default=448,\n",
    "                        help=\"The common width and height for all images\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1,\n",
    "                        help=\"The number of images per batch\")\n",
    "    # Training base Setting\n",
    "    parser.add_argument(\"--momentum\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--decay\", type=float, default=0.0005)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--num_epoches\", type=int, default=100)\n",
    "    parser.add_argument(\"--test_interval\", type=int, default=1,\n",
    "                        help=\"Number of epoches between testing phases\")\n",
    "    parser.add_argument(\"--object_scale\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--noobject_scale\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--class_scale\", type=float, default=1.0)\n",
    "    parser.add_argument(\"--coord_scale\", type=float, default=5.0)\n",
    "    parser.add_argument(\"--reduction\", type=int, default=32)\n",
    "    parser.add_argument(\"--es_min_delta\", type=float, default=0.0,\n",
    "                        help=\"Early stopping's parameter:minimum change loss to qualify as an improvement\")\n",
    "    parser.add_argument(\"--es_patience\", type=int, default=0,\n",
    "                        help=\"Early stopping's parameter:number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique.\")\n",
    "\n",
    "    parser.add_argument(\"--pre_trained_model_type\",\n",
    "                        type=str, choices=[\"model\", \"params\"],\n",
    "                        default=\"model\")\n",
    "    parser.add_argument(\"--pre_trained_model_path\", type=str,\n",
    "                        default=\"Yolo_v2_pytorch/trained_models/only_params_trained_yolo_voc\") # Pre-training path\n",
    "\n",
    "    parser.add_argument(\"--saved_path\", type=str,\n",
    "                        default=\"./checkpoint\") # saved training path\n",
    "    parser.add_argument(\"--conf_threshold\", type=float, default=0.35)\n",
    "    parser.add_argument(\"--nms_threshold\", type=float, default=0.5)\n",
    "    args = parser.parse_args(args=[]) # for jupyter \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, class_scale=1.0, conf_threshold=0.35, coord_scale=5.0, decay=0.0005, dropout=0.5, es_min_delta=0.0, es_patience=0, image_size=448, momentum=0.9, nms_threshold=0.5, noobject_scale=0.5, num_epoches=100, object_scale=1.0, pre_trained_model_path='Yolo_v2_pytorch/trained_models/only_params_trained_yolo_voc', pre_trained_model_type='model', reduction=32, saved_path='./checkpoint', test_interval=1)\n"
     ]
    }
   ],
   "source": [
    "opt = get_args()\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissOhDataset(Dataset):\n",
    "    def __init__(self, image_size=448):\n",
    "        img_path = '../data/AnotherMissOh/AnotherMissOh_images/AnotherMissOh01/'\n",
    "        json_dir = '../data/AnotherMissOh/AnotherMissOh_Visual/AnotherMissOh01_visual.json'\n",
    "\n",
    "        with open(json_dir, encoding='utf-8') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "        self.img_list = []\n",
    "        self.anno_list = []\n",
    "\n",
    "        for i in range(len(json_data['visual_results'])):\n",
    "            for j in range(len(json_data['visual_results'][i]['image_info'])):\n",
    "                label = []\n",
    "                for k in range(len(json_data['visual_results'][i]['image_info'][j]['persons'])):\n",
    "                    try:\n",
    "                        id_name = json_data['visual_results'][i]['image_info'][j]['persons'][k]['person_id']\n",
    "                        full_bbox = json_data['visual_results'][i]['image_info'][j]['persons'][k]['person_info'][\n",
    "                            'full_rect']\n",
    "                        if full_bbox['min_y'] == \"\" or full_bbox['max_y'] == \"\" or full_bbox['min_x'] == \"\" or full_bbox['max_x'] == \"\":\n",
    "                            continue\n",
    "                        else:\n",
    "                            temp_label = [full_bbox['min_x'], full_bbox['min_y'], full_bbox['max_x'], full_bbox['max_y'], 0]\n",
    "                            label.append(temp_label)\n",
    "                    except:\n",
    "                        id_name = []\n",
    "\n",
    "                self.img_list.append(img_path + json_data['visual_results'][i]['vid'][-9:].replace('_', '/') + '/' +\n",
    "                                     json_data['visual_results'][i]['image_info'][j]['frame_id'][-16:] + '.jpg')\n",
    "                self.anno_list.append(label)\n",
    "\n",
    "\n",
    "        self.image_size = (image_size, image_size)\n",
    "\n",
    "        self.num_images = len(self.img_list)\n",
    "        import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image_path = os.path.join(self.img_list[item])\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        transformations = Compose([Resize(self.image_size)])\n",
    "\n",
    "        objects = self.anno_list[item]\n",
    "        #\n",
    "        # image, objects = transformations((image, objects))\n",
    "        image = transformations(Image.fromarray(image))\n",
    "\n",
    "        return np.transpose(np.array(image, dtype=np.float32), (2, 0, 1)), np.array(objects, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json_data structure\n",
    "- json_data['file_name'] : 'AnotherMissOh01.mp4'\n",
    "- json_data['visual_results']\n",
    "- json_data['visual_results'][0].keys() : dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
    "- {\n",
    "'start_time': '00:02:51;16', \n",
    "'end_time': '00:02:54;15', \n",
    "'vid': 'AnotherMissOh01_001_0078', \n",
    "'image_info': ...}\n",
    "- json_data['visual_results'][0]['image_info']\n",
    "- [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', \n",
    "'place': 'none', \n",
    "'persons': [\n",
    "{'person_id': 'Haeyoung1', \n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, \n",
    "'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}], \n",
    "'objects': []}, \n",
    "- {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', \n",
    "'place': '', \n",
    "'persons': [{\n",
    "'person_id':'Haeyoung1',\n",
    "'person_info': {\n",
    "'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, \n",
    "'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, \n",
    "'behavior': 'stand up', \n",
    "'predicate': 'none', \n",
    "'emotion': 'Neutral', \n",
    "'face_rect_score': '0.5', \n",
    "'full_rect_score': '0.9'}, \n",
    "'related_objects': []}],\n",
    "'objects': []},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-13-2470c9be158d>\u001b[0m(36)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     35 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 36 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     37 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> json_data.keys()\n",
      "dict_keys(['file_name', 'visual_results'])\n",
      "ipdb> json_data.items()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> json_data['file_name']\n",
      "'AnotherMissOh01.mp4'\n",
      "ipdb> json_data['visual_results']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> json_data['visual_results'][0]\n",
      "{'start_time': '00:02:51;16', 'end_time': '00:02:54;15', 'vid': 'AnotherMissOh01_001_0078', 'image_info': [{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', 'place': 'none', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, 'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, 'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004303', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 829, 'max_y': 442}, 'full_rect': {'min_x': 255, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004319', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 514, 'min_y': 0, 'max_x': 836, 'max_y': 438}, 'full_rect': {'min_x': 255, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004327', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 835, 'max_y': 430}, 'full_rect': {'min_x': 288, 'min_y': 3, 'max_x': 1025, 'max_y': 769}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004335', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 543, 'min_y': 0, 'max_x': 849, 'max_y': 434}, 'full_rect': {'min_x': 288, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004343', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 573, 'min_y': 0, 'max_x': 900, 'max_y': 425}, 'full_rect': {'min_x': 321, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004351', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 614, 'min_y': 0, 'max_x': 941, 'max_y': 425}, 'full_rect': {'min_x': 362, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004359', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 640, 'min_y': 0, 'max_x': 967, 'max_y': 425}, 'full_rect': {'min_x': 363, 'min_y': 0, 'max_x': 1026, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}]}\n",
      "ipdb> json_data['visual_results'][0].keys()\n",
      "dict_keys(['start_time', 'end_time', 'vid', 'image_info'])\n",
      "ipdb> json_data['visual_results'][0]['image_info']\n",
      "[{'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004295', 'place': 'none', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 845, 'max_y': 443}, 'full_rect': {'min_x': 278, 'min_y': 2, 'max_x': 1025, 'max_y': 769}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004311', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 831, 'max_y': 411}, 'full_rect': {'min_x': 270, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004303', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 829, 'max_y': 442}, 'full_rect': {'min_x': 255, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004319', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 514, 'min_y': 0, 'max_x': 836, 'max_y': 438}, 'full_rect': {'min_x': 255, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004327', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 515, 'min_y': 0, 'max_x': 835, 'max_y': 430}, 'full_rect': {'min_x': 288, 'min_y': 3, 'max_x': 1025, 'max_y': 769}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.5', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004335', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 543, 'min_y': 0, 'max_x': 849, 'max_y': 434}, 'full_rect': {'min_x': 288, 'min_y': 0, 'max_x': 1022, 'max_y': 767}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004343', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 573, 'min_y': 0, 'max_x': 900, 'max_y': 425}, 'full_rect': {'min_x': 321, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004351', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 614, 'min_y': 0, 'max_x': 941, 'max_y': 425}, 'full_rect': {'min_x': 362, 'min_y': 0, 'max_x': 1025, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}, {'frame_id': 'AnotherMissOh01_001_0078_IMAGE_0000004359', 'place': '', 'persons': [{'person_id': 'Haeyoung1', 'person_info': {'face_rect': {'min_x': 640, 'min_y': 0, 'max_x': 967, 'max_y': 425}, 'full_rect': {'min_x': 363, 'min_y': 0, 'max_x': 1026, 'max_y': 768}, 'behavior': 'stand up', 'predicate': 'none', 'emotion': 'Neutral', 'face_rect_score': '0.6', 'full_rect_score': '0.9'}, 'related_objects': []}], 'objects': []}]\n",
      "ipdb> exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ff81f23c6eaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMissOhDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m training_params = {\"batch_size\": opt.batch_size,\n\u001b[1;32m      5\u001b[0m                    \u001b[0;34m\"shuffle\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-2470c9be158d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/vtt_env/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'return'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exception'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda/envs/vtt_env/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_return\u001b[0;34m(self, frame, arg)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_returning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;31m# The user issued a 'next' or 'until' command.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoplineno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_set = MissOhDataset(opt.image_size)\n",
    "\n",
    "\n",
    "training_params = {\"batch_size\": opt.batch_size,\n",
    "                   \"shuffle\": False,\n",
    "                   \"drop_last\": True}\n",
    "\n",
    "training_generator = DataLoader(training_set, **training_params)\n",
    "\n",
    "for iter, batch in enumerate(training_generator):\n",
    "    image, label = batch\n",
    "    \n",
    "    #----object bboxes--------\n",
    "    height, width = (768, 1024)\n",
    "    width_ratio = 1.0\n",
    "    height_ratio = 1.0\n",
    "    \n",
    "    image = image[0].cpu().numpy()\n",
    "    image = np.transpose(np.array(image, dtype=np.uint8), (1, 2, 0))\n",
    "    output_image = cv2.cvtColor(np.float32(image), cv2.COLOR_RGB2BGR)\n",
    "    output_image = cv2.resize(output_image, dsize=(1024, 768), interpolation=cv2.INTER_CUBIC)\n",
    "    print(image.shape)\n",
    "    for pred in label.cpu().numpy():\n",
    "        if len(pred) > 0:\n",
    "            pred = pred[0]\n",
    "            xmin = int(max(pred[0] / width_ratio, 0))\n",
    "            ymin = int(max(pred[1] / height_ratio, 0))\n",
    "            xmax = int(min((pred[0] + pred[2]) / width_ratio, width))\n",
    "            ymax = int(min((pred[1] + pred[3]) / height_ratio, height))\n",
    "            color = colors[int(pred[4])]\n",
    "\n",
    "            cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "            #text_size = cv2.getTextSize(pred[4] + ' : %.2f' % pred[4], cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n",
    "            #cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4),\n",
    "            #              color, -1)\n",
    "            #cv2.putText(output_image, pred[4] + ' : %.2f' % pred[4],\n",
    "            #    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n",
    "            #    (255, 255, 255), 1)\n",
    "            \n",
    "    #----image and bboxes-----\n",
    "    \n",
    "    output_image = np.uint8(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.imshow(np.uint8(output_image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
